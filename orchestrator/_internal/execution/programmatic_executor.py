"""
Programmatic Tool Calling Executor

Implements Anthropic's Programmatic Tool Calling pattern:
- LLM generates orchestration code
- Code executes in sandboxed environment with tool access
- Tool calls happen without LLM context pollution
- Only final results return to LLM

Benefits:
- 60-80% latency reduction (1 inference vs 20+)
- 37% token savings (intermediate data stays in sandbox)
- Parallel tool execution via asyncio.gather()
- Complex workflows (loops, filters, aggregations)
"""

import ast
import asyncio
import importlib
import json
import logging
import shutil
import sys
import tempfile
import time
import uuid
from collections.abc import Awaitable, Callable
from contextlib import redirect_stdout
from io import StringIO
from pathlib import Path
from typing import Any

import orchestrator.tools.tool_executor as tool_executor
from orchestrator.shared.models import ToolCatalog, ToolDefinition
from orchestrator.tools.tool_filesystem import ToolFileSystem

from .code_generator import StubGenerator
from .sandbox import ExecutionResult, ResourceLimits, create_sandbox


class SecurityError(Exception):
    """Raised when code violates security constraints"""
    pass


class ProgrammaticToolExecutor:
    """
    Execute LLM-generated code that orchestrates tool calls

    Example:
        executor = ProgrammaticToolExecutor(tool_catalog)

        code = '''
        team = await get_team_members("engineering")
        expenses = await asyncio.gather(*[get_expenses(m["id"], "Q3") for m in team])
        exceeded = [m["name"] for m, exp in zip(team, expenses) if sum(e["amount"] for e in exp) > 10000]
        print(json.dumps(exceeded))
        '''

        result = await executor.execute(code)
        print(result["output"])  # ["Alice", "Bob"]
        print(f"Called {len(result['tool_calls'])} tools in {result['execution_time']:.2f}s")
    """

    def __init__(
        self,
        tool_catalog: ToolCatalog,
        timeout: int = 30,
        max_tool_calls: int = 100,
        logger: logging.Logger | None = None,
        enable_stubs: bool = True,
        stub_dir: Path | None = None,
        use_sandbox: bool = True,
        sandbox_limits: ResourceLimits | None = None,
    ):
        """
        Args:
            tool_catalog: ToolCatalog instance with available tools
            timeout: Max execution time for user code
            max_tool_calls: Limit on tool calls per execution
            logger: Optional logger instance
            enable_stubs: If True, generate importable stubs for progressive disclosure
            stub_dir: Directory for generated stubs (auto-created if None)
            use_sandbox: Execute code inside sandbox with safety checks
            sandbox_limits: Optional resource limits for sandbox
        """
        self.tool_catalog = tool_catalog
        self.timeout = timeout
        self.max_tool_calls = max_tool_calls
        self.logger = logger or logging.getLogger(__name__)
        self.enable_stubs = enable_stubs
        self.use_sandbox = use_sandbox

        # Track tool calls for billing/monitoring
        self.tool_call_count = 0
        self.tool_call_log: list[dict[str, Any]] = []
        self.execution_id = f"ptc_{uuid.uuid4().hex[:8]}"

        # Phase 1: Stub generation for progressive disclosure
        self.stub_dir = stub_dir
        self.stubs_generated = False
        self._temp_dir: str | None = None

        if self.enable_stubs:
            self._prepare_stub_environment()

        # Initialize sandbox (Phase 2)
        self.sandbox = create_sandbox(use_docker=False, limits=sandbox_limits)

    async def execute(
        self,
        code: str,
        context: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        """
        Execute code that orchestrates tool calls

        Args:
            code: Python code generated by LLM
            context: Variables available to code (e.g., user_id, previous results)

        Returns:
            {
                "output": "...",  # stdout from code
                "result": {...},  # Final return value (if any)
                "tool_calls": [...],  # Tools called during execution
                "execution_time": 1.23,
                "error": None  # Or error message if failed
            }
        """
        start_time = time.time()
        self.tool_call_count = 0
        self.tool_call_log = []

        # Patch call_tool so generated stubs route back through this executor
        original_call_tool: Callable[[str, str, dict[str, Any], int], Awaitable[Any]] | None = None
        patched_call_tool = False
        if self.enable_stubs:
            try:
                original_call_tool = getattr(tool_executor, "call_tool", None)

                async def _call_tool_proxy(server: str, tool_name: str, parameters: dict[str, Any], timeout: int = 30) -> Any:
                    tool_def = self.tool_catalog.tools.get(tool_name)
                    if not tool_def:
                        raise ValueError(f"Tool not found: {tool_name}")

                    # Enforce limits
                    if self.tool_call_count >= self.max_tool_calls:
                        raise RuntimeError(
                            f"Exceeded max tool calls ({self.max_tool_calls}). "
                            f"Consider optimizing your code or increasing the limit."
                        )

                    # Track call
                    self.tool_call_count += 1
                    start_ts: float = time.time()
                    call_record = {
                        "tool": tool_def.name,
                        "type": tool_def.type,
                        "parameters": parameters,
                        "timestamp": start_ts,
                        "caller": {
                            "type": "stub_import",
                            "execution_id": self.execution_id,
                            "tool_id": f"tool_call_{self.tool_call_count}"
                        }
                    }
                    self.tool_call_log.append(call_record)

                    try:
                        result = await self._execute_tool(tool_def, parameters)
                        call_record["result_size"] = len(str(result))
                        completed_at: float = time.time()
                        call_record["completed_at"] = completed_at
                        call_record["duration"] = completed_at - start_ts  # Use local start_ts, not dict access
                        call_record["error"] = None
                        return result
                    except Exception as e:
                        call_record["error"] = str(e)
                        completed_at = time.time()
                        call_record["completed_at"] = completed_at
                        call_record["duration"] = completed_at - start_ts  # Use local start_ts, not dict access
                        raise

                tool_executor.call_tool = _call_tool_proxy
                patched_call_tool = True
            except Exception as patch_exc:
                self.logger.warning(f"{self.execution_id}: Failed to patch call_tool for stubs: {patch_exc}")

        # Build execution environment / context
        exec_context = {
            "json": json,
            **(context or {})
        }
        # Provide sys for safe path adjustments inside sandbox (no import needed)
        exec_context["sys"] = sys

        # Inject tool functions
        for tool_name, tool_def in self.tool_catalog.tools.items():
            exec_context[tool_name] = self._create_tool_wrapper(tool_def)

        try:
            if self.use_sandbox:
                # Wrap code with prelude to ensure stub paths are importable, then async __main__ for sandbox
                prelude_lines = []
                try:
                    if self.stub_dir is not None:
                        # Ensure both the stub root and the tools package are importable
                        prelude_lines.append(f"sys.path.insert(0, r'{str(self.stub_dir)}')")
                        prelude_lines.append(f"sys.path.insert(0, r'{str(self.stub_dir / 'tools')}')")
                        prelude_lines.append("import importlib")
                        prelude_lines.append("importlib.invalidate_caches()")
                        # Evict any pre-existing 'tools' modules not coming from our stub_dir
                        prelude_lines.append("try:")
                        prelude_lines.append("    _sd = r'" + str(self.stub_dir).replace("\\", "\\\\") + "'")
                        prelude_lines.append("    if 'tools' in sys.modules:")
                        prelude_lines.append("        _m = sys.modules['tools']")
                        prelude_lines.append("        _p = getattr(_m, '__file__', None)")
                        prelude_lines.append("        _pp = getattr(_m, '__path__', None)")
                        prelude_lines.append("        _loc = str(_p or (_pp[0] if _pp else ''))")
                        prelude_lines.append("        if _sd not in _loc:")
                        prelude_lines.append("            del sys.modules['tools']")
                        prelude_lines.append("    for _k in list(sys.modules.keys()):")
                        prelude_lines.append("        if _k.startswith('tools.'):")
                        prelude_lines.append("            del sys.modules[_k]")
                        prelude_lines.append("except Exception:")
                        prelude_lines.append("    pass")
                except Exception:
                    pass
                user_body = "\n".join(
                    f"    {line}" if line.strip() else "" for line in code.split("\n")
                )
                wrapped_code = "\n".join(prelude_lines + ["async def __main__():", user_body])

                # Align sandbox timeout with executor timeout
                if hasattr(self.sandbox, "limits") and self.sandbox.limits:
                    try:
                        self.sandbox.limits.max_duration = float(self.timeout)
                    except Exception:
                        pass

                # Execute in sandbox (sandbox enforces its own timeout)
                sres: ExecutionResult = await self.sandbox.execute(wrapped_code, exec_context)
                execution_time = time.time() - start_time

                self.logger.info(
                    f"Programmatic execution {self.execution_id}: "
                    f"{self.tool_call_count} tools called in {execution_time:.2f}s"
                )

                # Normalize error format to match legacy expectations
                if sres.success:
                    error_val = None
                else:
                    if sres.error_type == "SecurityError":
                        msg = sres.error or "Sandbox security violation"
                        # Ensure 'SyntaxError' token is present for syntax errors
                        if isinstance(msg, str) and "Syntax error" in msg:
                            msg = "SyntaxError: " + msg.split("Syntax error:", 1)[-1].strip()
                        error_val = f"Security violation: {msg}"
                    elif sres.error_type:
                        if sres.error:
                            error_val = f"{sres.error_type}: {sres.error}"
                        else:
                            error_val = sres.error_type
                    else:
                        error_val = sres.error or "Execution failed"

                return {
                    "output": sres.stdout,
                    "result": sres.output,
                    "tool_calls": self.tool_call_log,
                    "execution_time": execution_time,
                    "error": error_val,
                    "execution_id": self.execution_id
                }
            else:
                # Legacy in-process execution path
                exec_globals = {
                    "__builtins__": self._get_safe_builtins(),
                    "asyncio": asyncio,
                    "json": json,
                    **(context or {})
                }
                for tool_name, tool_def in self.tool_catalog.tools.items():
                    exec_globals[tool_name] = self._create_tool_wrapper(tool_def)

                stdout_buffer = StringIO()
                parsed = ast.parse(code)
                self._validate_code_safety(parsed)
                with redirect_stdout(stdout_buffer):
                    result = await asyncio.wait_for(
                        self._exec_async(code, exec_globals),
                        timeout=self.timeout
                    )
                execution_time = time.time() - start_time
                self.logger.info(
                    f"Programmatic execution {self.execution_id}: "
                    f"{self.tool_call_count} tools called in {execution_time:.2f}s"
                )
                return {
                    "output": stdout_buffer.getvalue() if 'stdout_buffer' in locals() else "",
                    "result": result,
                    "tool_calls": self.tool_call_log,
                    "execution_time": execution_time,
                    "error": None,
                    "execution_id": self.execution_id
                }

        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            error_msg = f"Execution timeout after {self.timeout}s"
            self.logger.error(f"{self.execution_id}: {error_msg}")

            return {
                "output": stdout_buffer.getvalue() if 'stdout_buffer' in locals() else "",
                "result": None,
                "tool_calls": self.tool_call_log,
                "execution_time": execution_time,
                "error": error_msg,
                "execution_id": self.execution_id
            }

        except SecurityError as e:
            execution_time = time.time() - start_time
            error_msg = f"Security violation: {str(e)}"
            self.logger.error(f"{self.execution_id}: {error_msg}")

            return {
                "output": stdout_buffer.getvalue() if 'stdout_buffer' in locals() else "",
                "result": None,
                "tool_calls": self.tool_call_log,
                "execution_time": execution_time,
                "error": error_msg,
                "execution_id": self.execution_id
            }

        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"{type(e).__name__}: {str(e)}"
            self.logger.error(f"{self.execution_id}: {error_msg}")

            return {
                "output": stdout_buffer.getvalue() if 'stdout_buffer' in locals() else "",
                "result": None,
                "tool_calls": self.tool_call_log,
                "execution_time": execution_time,
                "error": error_msg,
                "execution_id": self.execution_id
            }

        finally:
            if self.enable_stubs and patched_call_tool:
                try:
                    if original_call_tool is not None:
                        tool_executor.call_tool = original_call_tool  # type: ignore[assignment]
                except Exception as restore_exc:
                    self.logger.warning(f"{self.execution_id}: Failed to restore call_tool: {restore_exc}")

    def _create_tool_wrapper(self, tool_def: ToolDefinition) -> Callable[..., Awaitable[Any]]:
        """
        Create async function that wraps tool execution

        Example:
            tool_def.name = "get_expenses"
            Returns: async def get_expenses(user_id, quarter): ...

        The wrapper:
        - Validates parameters
        - Tracks tool calls
        - Logs caller context (Anthropic pattern)
        - Executes the actual tool
        - Returns result directly
        """
        async def tool_wrapper(**kwargs: Any) -> Any:
            # Validate parameters
            required_params = [p.name for p in tool_def.parameters if p.required]
            missing = set(required_params) - set(kwargs.keys())
            if missing:
                raise ValueError(f"{tool_def.name}: Missing required parameters: {missing}")

            # Check tool call limit
            if self.tool_call_count >= self.max_tool_calls:
                raise RuntimeError(
                    f"Exceeded max tool calls ({self.max_tool_calls}). "
                    f"Consider optimizing your code or increasing the limit."
                )

            self.tool_call_count += 1

            # Log the call with caller information (Anthropic pattern)
            start_ts: float = time.time()
            call_record = {
                "tool": tool_def.name,
                "type": tool_def.type,
                "parameters": kwargs,
                "timestamp": start_ts,
                "caller": {
                    "type": "code_execution",
                    "execution_id": self.execution_id,
                    "tool_id": f"tool_call_{self.tool_call_count}"
                }
            }
            self.tool_call_log.append(call_record)

            # Execute the actual tool
            try:
                result = await self._execute_tool(tool_def, kwargs)

                call_record["result_size"] = len(str(result))
                end_ts: float = time.time()
                call_record["completed_at"] = end_ts
                call_record["duration"] = end_ts - start_ts
                call_record["error"] = None

                return result

            except Exception as e:
                call_record["error"] = str(e)
                end_ts = time.time()
                call_record["completed_at"] = end_ts
                call_record["duration"] = end_ts - start_ts
                raise

        # Set function name for better debugging
        tool_wrapper.__name__ = tool_def.name
        return tool_wrapper

    async def _execute_tool(
        self,
        tool_def: ToolDefinition,
        parameters: dict[str, Any]
    ) -> Any:
        """
        Execute the actual tool based on its type

        Note: We import inside functions to avoid circular dependencies
        """
        if tool_def.type == "mcp":
            # Call MCP worker
            from orchestrator._internal.infra.mcp_client import MCPClientShim

            shim = MCPClientShim()
            if tool_def.name not in shim.tool_map:
                raise ValueError(f"MCP tool not found: {tool_def.name}")

            tool_func = shim.tool_map[tool_def.name]
            result = await tool_func(parameters)
            return result

        elif tool_def.type == "function":
            # Call structured function via hybrid dispatcher worker
            from orchestrator._internal.dispatch.hybrid_dispatcher import function_call_worker
            payload = {
                "name": tool_def.name,
                "args": parameters,
            }
            result = await function_call_worker(payload)
            return result

        elif tool_def.type == "code_exec":
            # Nested code execution not supported (prevents recursion attacks)
            raise SecurityError(
                "Nested code execution is not allowed. "
                "Move code_exec logic into programmatic calling instead."
            )

        else:
            raise ValueError(f"Unknown tool type: {tool_def.type}")

    async def _exec_async(self, code: str, exec_globals: dict[str, Any]) -> Any:
        """
        Execute code that may contain await statements

        Wraps code in an async function to support await statements at top level
        """
        # Wrap code in async function
        wrapped_code = "async def __exec_func():\n"
        wrapped_code += "\n".join(f"    {line}" for line in code.split("\n"))

        # Execute wrapper definition
        exec(wrapped_code, exec_globals)

        # Call the async function
        return await exec_globals["__exec_func"]()

    def _validate_code_safety(self, parsed: ast.AST) -> None:
        """
        AST-based code validation for security

        Blocks:
        - File I/O (open, write, Path)
        - Network calls (socket, requests, urllib)
        - Process spawning (subprocess, os.system)
        - Dangerous imports (pickle, marshal, ctypes)
        - Code execution (eval, exec, compile)
        - Reflection abuse (__import__, getattr with strings)
        - Environment manipulation (os.environ)

        Production Note:
        For maximum security, consider additional sandboxing:
        - Docker containers (full isolation)
        - seccomp filters (Linux syscall filtering)
        - Resource limits (memory, CPU, disk)
        """
        forbidden_imports = [
            "pickle", "marshal", "subprocess", "os", "sys",
            "ctypes", "importlib", "__builtin__", "builtins",
            "socket", "urllib", "requests", "http", "httpx",
            "pty", "fcntl", "resource", "signal",
            "pathlib", "shutil", "tempfile"
        ]

        forbidden_functions = [
            "eval", "exec", "compile", "__import__",
            "open", "input", "raw_input",
            "delattr", "setattr", "getattr",  # Prevent reflection abuse
            "globals", "locals", "vars", "dir"  # Prevent introspection abuse
        ]

        for node in ast.walk(parsed):
            # Check imports
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias.name in forbidden_imports:
                        raise SecurityError(f"Forbidden import: {alias.name}")

                    # Block partial matches (e.g., "os.path")
                    base_module = alias.name.split(".")[0]
                    if base_module in forbidden_imports:
                        raise SecurityError(f"Forbidden import: {alias.name}")

            # Check from X import Y
            if isinstance(node, ast.ImportFrom):
                if node.module:
                    if node.module in forbidden_imports:
                        raise SecurityError(f"Forbidden import: from {node.module}")

                    # Block partial matches
                    base_module = node.module.split(".")[0]
                    if base_module in forbidden_imports:
                        raise SecurityError(f"Forbidden import: from {node.module}")

            # Check function calls
            if isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    if node.func.id in forbidden_functions:
                        raise SecurityError(f"Forbidden function: {node.func.id}()")

                # Check for dangerous attribute access: obj.system()
                if isinstance(node.func, ast.Attribute):
                    if node.func.attr in ["system", "popen", "spawn", "call", "run"]:
                        raise SecurityError(f"Forbidden method: .{node.func.attr}()")

            # Check for attempts to modify __builtins__
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        if target.id in ["__builtins__", "__globals__", "__dict__"]:
                            raise SecurityError(f"Cannot modify: {target.id}")

    def _get_safe_builtins(self) -> dict[str, Any]:
        """
        Return safe subset of builtins

        Only includes functions safe for data manipulation:
        - Collection operations (len, range, zip, etc.)
        - Math operations (sum, min, max, etc.)
        - Type conversions (str, int, float, etc.)
        - Data structures (list, dict, set, tuple)
        - Exception classes (for error handling)

        Explicitly excludes:
        - File I/O (open)
        - Code execution (eval, exec, compile)
        - Introspection (globals, locals, vars)
        - Input (input, raw_input)
        """
        safe = {
            # Iteration
            "range", "enumerate", "zip", "map", "filter", "iter", "next",
            # Collections
            "len", "list", "dict", "set", "tuple", "frozenset",
            # Math
            "sum", "min", "max", "abs", "round", "pow", "divmod",
            # Strings
            "str", "repr", "format", "ord", "chr",
            # Numbers
            "int", "float", "complex", "bool",
            # Sorting
            "sorted", "reversed",
            # Type checking
            "type", "isinstance", "issubclass",
            # Output
            "print",
            # Constants
            "True", "False", "None",
            # Exception classes (for error handling)
            "Exception", "ValueError", "TypeError", "KeyError", "IndexError",
            "RuntimeError", "AttributeError", "ZeroDivisionError",
            # Other safe operations
            "all", "any", "slice", "hash", "id"
        }

        # Build dictionary from __builtins__
        if isinstance(__builtins__, dict):
            return {k: __builtins__[k] for k in safe if k in __builtins__}
        else:
            return {k: getattr(__builtins__, k) for k in safe if hasattr(__builtins__, k)}

    def _prepare_stub_environment(self) -> None:
        """
        Generate tool stubs and prepare for progressive disclosure.

        Phase 1: Code Execution with Progressive Disclosure
        - Generates Python stubs from ToolCatalog
        - Makes stubs importable via sys.path
        - AI models can explore file tree instead of loading full catalog
        - 30-50% context reduction
        """
        try:
            # Create stub directory
            if self.stub_dir is None:
                temp_path = tempfile.mkdtemp(prefix="toolweaver_stubs_")
                self._temp_dir = temp_path
                self.stub_dir = Path(temp_path)
            else:
                # stub_dir provided by caller, ensure it's a Path object
                stub_path = self.stub_dir if isinstance(self.stub_dir, Path) else Path(self.stub_dir)
                self.stub_dir = stub_path
                self.stub_dir.mkdir(parents=True, exist_ok=True)

            # Generate stubs
            self.logger.info(f"Generating tool stubs in {self.stub_dir}")
            generator = StubGenerator(self.tool_catalog, self.stub_dir)
            stubs = generator.generate_all()

            self.logger.info(f"Generated {len(stubs)} tool stubs")

            # Add to sys.path for importing (only stub root; contains 'tools' package)
            stub_path_str = str(self.stub_dir)
            if stub_path_str not in sys.path:
                sys.path.insert(0, stub_path_str)
                self.logger.debug(f"Added {stub_path_str} to sys.path")

            # Create ToolFileSystem for exploration
            self.tool_filesystem = ToolFileSystem(self.stub_dir)

            # Eagerly import generated stub modules so sandbox imports resolve from sys.modules
            try:
                importlib.invalidate_caches()
                # Ensure top-level 'tools' package is loaded
                try:
                    importlib.import_module("tools")
                except Exception:
                    pass
                for tool_def in self.tool_catalog.tools.values():
                    server = tool_def.domain or "general"
                    modname = f"tools.{server}.{tool_def.name}"
                    try:
                        importlib.import_module(modname)
                        self.logger.debug(f"Imported stub module: {modname}")
                    except Exception as im_err:
                        self.logger.debug(f"Could not import stub module {modname}: {im_err}")
            except Exception as imp_all_err:
                self.logger.debug(f"Stub pre-import pass failed: {imp_all_err}")

            self.stubs_generated = True
            self.logger.info("Stub environment ready for progressive disclosure")

        except Exception as e:
            self.logger.warning(f"Failed to prepare stub environment: {e}")
            self.logger.warning("Falling back to direct tool injection")
            self.enable_stubs = False

    def get_tools_directory_tree(self) -> str | None:
        """
        Get visual representation of tool directory for AI exploration.

        Returns:
            Directory tree string, or None if stubs not enabled

        Usage in prompts:
            "Available tools are organized in this directory:
            {tree}

            You can import tools like:
            from tools.google_drive import get_document, GetDocumentInput"
        """
        if not self.enable_stubs or not hasattr(self, 'tool_filesystem'):
            return None

        return self.tool_filesystem.get_directory_tree()

    def search_tools(self, query: str) -> list[str]:
        """
        Search for tools by name or description.

        Args:
            query: Search query

        Returns:
            List of matching tool names

        Usage: Allow AI to search tools before importing
        """
        if not self.enable_stubs or not hasattr(self, 'tool_filesystem'):
            # Fallback to catalog keys
            return [name for name in self.tool_catalog.tools.keys() if query.lower() in name.lower()]

        return self.tool_filesystem.search_tools(query)

    def cleanup(self) -> None:
        """Clean up temporary stub directory"""
        if self._temp_dir:
            try:
                # Remove from sys.path
                if self.stub_dir and sys.path:
                    stub_path_str = str(self.stub_dir)
                    tools_path_str = str(self.stub_dir / "tools")
                    for path_str in (stub_path_str, tools_path_str):
                        try:
                            if path_str in sys.path:
                                sys.path.remove(path_str)
                        except (TypeError, ValueError):
                            pass

                # Delete temp directory
                shutil.rmtree(self._temp_dir, ignore_errors=True)
                self.logger.debug(f"Cleaned up stub directory: {self._temp_dir}")
            except Exception as e:
                # Suppress noisy shutdown errors when interpreter is tearing down
                try:
                    self.logger.warning(f"Failed to cleanup stub directory: {e}")
                except Exception:
                    pass

    def __del__(self) -> None:
        """Cleanup on deletion"""
        self.cleanup()


# Convenience function for quick usage
async def execute_programmatic_code(
    code: str,
    tool_catalog: ToolCatalog,
    context: dict[str, Any] | None = None,
    timeout: int = 30
) -> dict[str, Any]:
    """
    Convenience function to execute programmatic tool calling code

    Example:
        result = await execute_programmatic_code(
            code="team = await get_team_members('engineering'); print(len(team))",
            tool_catalog=catalog
        )
        print(result["output"])  # "15"
    """
    executor = ProgrammaticToolExecutor(tool_catalog, timeout=timeout)
    return await executor.execute(code, context)
