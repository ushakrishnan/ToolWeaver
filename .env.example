# ============================================================
# AZURE COMPUTER VISION CONFIGURATION (for OCR)
# ============================================================

AZURE_CV_ENDPOINT=https://your-resource-name.cognitiveservices.azure.com/

# Authentication Method:
# Option 1: API Key (if local auth is enabled)
AZURE_CV_KEY=your-azure-computer-vision-key

# Option 2: Azure AD (if disableLocalAuth is true)
# Set this to 'true' and remove/comment AZURE_CV_KEY
# AZURE_USE_AD=true

# OCR Mode: 'mock' for fake data, 'azure' for real Azure CV
OCR_MODE=mock


# ============================================================
# LARGE MODEL PLANNER
# ============================================================

# Provider: openai, azure-openai, anthropic, or gemini
PLANNER_PROVIDER=azure-openai

# Model name (deployment name for Azure OpenAI)
PLANNER_MODEL=gpt-4o

# For OpenAI (GPT-4o, GPT-4-turbo):
# OPENAI_API_KEY=your-openai-api-key

# For Azure OpenAI:
# Option 1: API Key (if local auth is enabled)
#AZURE_OPENAI_API_KEY=your-azure-openai-key

# Option 2: Azure AD (if disableLocalAuth is true) - RECOMMENDED
# Set this to 'true' for Azure AD authentication
AZURE_OPENAI_USE_AD=true

AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-08-01-preview

# For OpenAI (if using PLANNER_PROVIDER=openai):
OPENAI_API_KEY=sk-your-key-here

# For Anthropic (if using PLANNER_PROVIDER=anthropic):
ANTHROPIC_API_KEY=sk-ant-your-key-here

# For Google Gemini (if using PLANNER_PROVIDER=gemini):
GOOGLE_API_KEY=your-google-api-key


# ============================================================
# SMALL MODEL WORKERS (Phi-3, Llama, etc.)
# ============================================================

# Enable small model workers for parsing and categorization
USE_SMALL_MODEL=false

# Backend options: 'ollama' (local), 'transformers' (local), 'azure' (cloud)
SMALL_MODEL_BACKEND=ollama

# Model name
# - For Ollama: 'phi3', 'llama3.2', 'mistral'
# - For Transformers: 'microsoft/Phi-3-mini-4k-instruct'
# - For Azure: deployment name or model name
WORKER_MODEL=phi3

# Ollama Configuration (if SMALL_MODEL_BACKEND=ollama)
OLLAMA_API_URL=http://localhost:11434

# Azure AI Foundry / Azure OpenAI Configuration (if SMALL_MODEL_BACKEND=azure)
# AZURE_SMALL_MODEL_ENDPOINT=https://your-foundry-endpoint.inference.ml.azure.com/score
# AZURE_SMALL_MODEL_KEY=your-foundry-key
# Or use Azure AD authentication:
# AZURE_SMALL_MODEL_USE_AD=true

# ============================================================
# PHASE 7: VECTOR DATABASE (Qdrant) - For 1000+ Tools
# ============================================================

# Qdrant URL - Choose one option:

# Option 1: Local Docker (FREE)
QDRANT_URL=http://localhost:6333

# Option 2: Local WSL (FREE)
# QDRANT_URL=http://172.xx.xx.xx:6333

# Option 3: Qdrant Cloud Free Tier (FREE, 1 GB, 100k vectors) - RECOMMENDED
# QDRANT_URL=https://xxxxx.us-east-1-0.aws.cloud.qdrant.io
# QDRANT_API_KEY=your-qdrant-cloud-api-key

# Option 4: Azure Container Instances (Self-hosted, ~$30/month)
# QDRANT_URL=http://toolweaver-qdrant.eastus.azurecontainer.io:6333

# API Key (required for Qdrant Cloud, optional for local)
QDRANT_API_KEY=

# Collection name for tool embeddings
QDRANT_COLLECTION=toolweaver_tools


# ============================================================
# PHASE 7: GPU ACCELERATION - For Fast Embedding Generation
# ============================================================

# Use GPU for embedding generation (if available)
# Automatically detects CUDA (NVIDIA) or MPS (Apple Silicon)
# Set to false to force CPU-only mode
USE_GPU=true

# Pre-compute embeddings at startup to eliminate cold-start latency
# Caches embeddings in memory for ~11s â†’ <100ms improvement
PRECOMPUTE_EMBEDDINGS=true

# Embedding batch size (automatically adjusted for GPU: CPU=32, GPU=128)
EMBEDDING_BATCH_SIZE=32


# ============================================================
# PHASE 7: DISTRIBUTED CACHE (Redis) - For Multiple Instances
# ============================================================

# Redis URL - Choose one option:

# Option 1: Local Docker (FREE)
REDIS_URL=redis://localhost:6379

# Option 2: Local WSL (FREE)
# REDIS_URL=redis://172.xx.xx.xx:6379

# Option 3: Redis Cloud Free Tier (FREE, 30 MB) - RECOMMENDED
# REDIS_URL=redis://redis-12345.c123.us-east-1-2.ec2.cloud.redislabs.com:12345
# REDIS_PASSWORD=your-redis-cloud-password

# Option 4: Azure Cache for Redis (Basic C0: $18/mo, Standard C1: $45/mo)
# REDIS_URL=rediss://toolweaver-cache.redis.cache.windows.net:6380
# REDIS_PASSWORD=your-azure-redis-primary-key

# Redis Password (required for cloud providers, empty for local)
REDIS_PASSWORD=

# Enable fallback to file cache if Redis unavailable
REDIS_FALLBACK_ENABLED=true


# ============================================================
# MONITORING & OBSERVABILITY
# ============================================================

# Monitoring Backends: local, wandb, prometheus (comma-separated for multiple)
# - local: File-based logging (default, no dependencies)
# - wandb: Weights & Biases integration (optional, requires: pip install wandb)
# - prometheus: Prometheus metrics export (optional, requires: pip install prometheus-client)
MONITORING_BACKENDS=local

# Local Backend (default, zero dependencies)
# Directory for log files (JSONL format, daily rotation)
TOOL_LOGS_DIR=.tool_logs

# ============================================================
# WEIGHTS & BIASES (W&B) - Optional
# ============================================================
# Get API key from: https://wandb.ai/settings
# Sign up at: https://wandb.ai/site
# Free tier: Unlimited runs, 100 GB storage
# Team tier: $50/user/month

# Required for W&B backend
# WANDB_API_KEY=your-wandb-api-key-here

# W&B Project Configuration
# WANDB_PROJECT=toolweaver
# WANDB_ENTITY=your-team-name
# WANDB_RUN_NAME=production-run-1

# Features enabled with W&B:
# - Beautiful dashboards for tool usage, latency, errors
# - Experiment comparison (A/B test prompts, models, configs)
# - Team collaboration and shared metrics
# - Version tracking for prompts and configs

# ============================================================
# PROMETHEUS - Optional
# ============================================================
# Metrics endpoint for Prometheus scraping
# Prometheus server will scrape: http://localhost:8000/metrics
# Requires: pip install prometheus-client

# Port for Prometheus metrics HTTP server
# PROMETHEUS_PORT=8000

# Metrics exposed:
# - toolweaver_tool_calls_total (counter)
# - toolweaver_tool_errors_total (counter)
# - toolweaver_tool_latency_seconds (histogram)
# - toolweaver_search_queries_total (counter)
# - toolweaver_cache_hits_total (counter)
# - toolweaver_tokens_total (counter)

# Example Prometheus scrape config (prometheus.yml):
# scrape_configs:
#   - job_name: 'toolweaver'
#     static_configs:
#       - targets: ['localhost:8000']

# Grafana Dashboard available at: https://grafana.com/grafana/dashboards/
# Search for: "Prometheus Python" or create custom dashboard


# ============================================================
# MONITORING USAGE EXAMPLES
# ============================================================

# Example 1: Local only (default, no setup needed)
# MONITORING_BACKENDS=local
# Logs to: .tool_logs/tool_calls_2025-12-16.jsonl

# Example 2: W&B for experiment tracking
# MONITORING_BACKENDS=wandb
# WANDB_API_KEY=your-key
# WANDB_PROJECT=my-toolweaver-project

# Example 3: Prometheus for production
# MONITORING_BACKENDS=prometheus
# PROMETHEUS_PORT=8000

# Example 4: All backends (development + staging + production)
# MONITORING_BACKENDS=local,wandb,prometheus
# WANDB_API_KEY=your-key
# PROMETHEUS_PORT=8000