"""
Simple usage examples for the two-model orchestrator.
"""

import asyncio

from orchestrator.orchestrator import execute_plan
from orchestrator.planner import LargePlanner
from orchestrator.small_model_worker import SmallModelWorker

# ============================================================
# Example 1: Generate Plan from Natural Language
# ============================================================

async def example_1_natural_language():
    """Use GPT-4o to generate a plan from natural language."""
    print("\n" + "="*60)
    print("EXAMPLE 1: Natural Language ‚Üí Execution Plan")
    print("="*60 + "\n")

    # Initialize large model planner
    planner = LargePlanner(provider="openai")

    # User request in natural language
    request = "Process this grocery receipt and tell me how much I spent on food vs beverages"

    # Context (optional)
    context = {
        "image_url": "https://example.com/receipt.jpg"
    }

    # Generate plan
    plan = await planner.generate_plan(request, context=context)

    print(f"Generated {len(plan['steps'])} steps:")
    for step in plan['steps']:
        print(f"  - {step['id']}: {step['tool']}")

    # Execute the plan
    result = await execute_plan(plan)
    print(f"\n[OK] Completed! {len(result['step_outputs'])} steps executed")

    return plan, result


# ============================================================
# Example 2: Small Model Worker for Text Processing
# ============================================================

async def example_2_small_model():
    """Use Phi-3 to parse and categorize receipt items."""
    print("\n" + "="*60)
    print("EXAMPLE 2: Small Model (Phi-3) for Parsing")
    print("="*60 + "\n")

    # Initialize small model worker
    worker = SmallModelWorker(backend="ollama", model_name="phi3")

    # Sample receipt text
    receipt_text = """
    Coffee Shop Receipt

    1x Espresso           $3.50
    2x Croissant          $8.00
    1x Orange Juice       $4.50

    Subtotal: $16.00
    Tax: $1.12
    Total: $17.12
    """

    # Parse line items with Phi-3
    print("ü§ñ Phi-3 parsing receipt...")
    items = await worker.parse_line_items(receipt_text)
    print(f"[OK] Found {len(items)} items:")
    for item in items:
        print(f"  - {item['description']}: ${item['total']}")

    # Categorize with Phi-3
    print("\nü§ñ Phi-3 categorizing items...")
    categorized = await worker.categorize_items(items)
    print(f"[OK] Categorized {len(categorized)} items:")
    for item in categorized:
        print(f"  - {item['description']}: {item.get('category', 'unknown')}")

    return items, categorized


# ============================================================
# Example 3: Cost Comparison
# ============================================================

async def example_3_cost_comparison():
    """Compare costs: Large model only vs Two-model architecture."""
    print("\n" + "="*60)
    print("EXAMPLE 3: Cost Comparison")
    print("="*60 + "\n")

    num_receipts = 1000

    # Scenario 1: GPT-4o for everything
    gpt4o_per_call = 0.015  # $0.015 per receipt (OCR + parsing + categorization)
    gpt4o_total = num_receipts * gpt4o_per_call

    # Scenario 2: Two-model architecture
    planning_cost = 0.002  # One planning call (amortized across batch)
    phi3_cost = 0.0  # Free (local inference)
    two_model_total = planning_cost

    print(f"Processing {num_receipts} receipts:\n")
    print("GPT-4o Only:")
    print(f"  - Cost per receipt: ${gpt4o_per_call}")
    print(f"  - Total cost: ${gpt4o_total:.2f}")
    print("  - Latency: ~5-10s per receipt")

    print("\nTwo-Model (GPT-4o + Phi-3):")
    print(f"  - Planning: ${planning_cost:.3f} (one-time)")
    print(f"  - Execution: ${phi3_cost} (local Phi-3)")
    print(f"  - Total cost: ${two_model_total:.3f}")
    print("  - Latency: ~2-4s per receipt")

    savings = ((gpt4o_total - two_model_total) / gpt4o_total) * 100
    print(f"\nüí∞ Savings: ${gpt4o_total - two_model_total:.2f} ({savings:.1f}%)")
    print("‚ö° Speed improvement: ~2x faster")


# ============================================================
# Example 4: Hybrid Plan with All Tool Types
# ============================================================

async def example_4_hybrid_plan():
    """Execute a plan using all three tool types."""
    print("\n" + "="*60)
    print("EXAMPLE 4: Hybrid Plan (MCP + Functions + Code)")
    print("="*60 + "\n")

    # Manual plan (or could be generated by large model)
    plan = {
        "request_id": "example-4",
        "steps": [
            {
                "id": "step-1",
                "tool": "receipt_ocr",  # MCP worker
                "input": {"image_uri": "https://example.com/receipt.jpg"},
                "depends_on": []
            },
            {
                "id": "step-2",
                "tool": "line_item_parser",  # MCP worker (uses Phi-3 if enabled)
                "input": {"ocr_text": "step:step-1"},
                "depends_on": ["step-1"]
            },
            {
                "id": "step-3",
                "tool": "function_call",  # Structured function
                "input": {
                    "name": "compute_item_statistics",
                    "args": {"items": "step:step-2"}
                },
                "depends_on": ["step-2"]
            },
            {
                "id": "step-4",
                "tool": "code_exec",  # Sandboxed Python
                "input": {
                    "code": "output = {'total_items': len(input['items'])}",
                    "input_data": {"items": "step:step-2"}
                },
                "depends_on": ["step-2"]
            }
        ]
    }

    print("Plan uses:")
    print("  ‚úì MCP Workers (OCR, parsing)")
    print("  ‚úì Function Calls (statistics)")
    print("  ‚úì Code Execution (custom logic)")

    # Execute
    result = await execute_plan(plan)
    print(f"\n[OK] Executed {len(result['step_outputs'])} steps")

    return plan, result


# ============================================================
# Run All Examples
# ============================================================

async def main():
    """Run all examples."""
    print("\n" + "="*60)
    print("TWO-MODEL ORCHESTRATOR - USAGE EXAMPLES")
    print("="*60)

    # Example 3 doesn't need API keys
    await example_3_cost_comparison()

    # Examples 1, 2, 4 require configuration
    print("\n\n‚ÑπÔ∏è  Examples 1, 2, 4 require configuration:")
    print("  - Example 1: Set OPENAI_API_KEY in .env")
    print("  - Example 2: Install Ollama and run 'ollama pull phi3'")
    print("  - Example 4: Works with default setup")

    print("\n\nTo run all examples:")
    print("  1. Configure .env with API keys")
    print("  2. Install Ollama: https://ollama.ai")
    print("  3. Run: python usage_examples.py")


if __name__ == "__main__":
    asyncio.run(main())
