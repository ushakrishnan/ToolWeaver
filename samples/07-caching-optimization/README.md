# Example 07: Caching and Optimization

**Complexity:** ‚≠ê‚≠ê Intermediate | **Time:** 15-20 minutes  
**Feature Demonstrated:** Multi-layer caching with Redis for cost and performance optimization

## üí° THE SOUL: Why Caching Matters

**The Problem You Solve:**
- LLM API calls are expensive ($0.01-$0.10 each)
- Tool discovery runs repeatedly on similar queries
- Vector embeddings cost $0.002-$0.005 per request
- Every repeated operation wastes money & time

**The ToolWeaver Solution:**
Cache intelligently so you pay once, use many times:
- ‚úÖ **Same query = instant response** (0ms vs 800ms)
- ‚úÖ **80% cost reduction** ($0.26 ‚Üí $0.09 per workflow)
- ‚úÖ **1.5-1.6x speedup** for repeated operations
- ‚úÖ **Zero downtime** with automatic Redis ‚Üí file fallback

**Real Impact:**
```
Without Caching: 100 workflows √ó $0.26 = $26.00 + 2+ hours latency
With Caching:    100 workflows √ó $0.09 = $9.00  + 10 minutes latency
                 SAVE $17.00 (65%) + 6x faster
```

---

## Overview

### What This Example Does
Demonstrates caching strategies from simple in-memory patterns to distributed Redis caching with automatic fallback, circuit breaker protection, and optimized TTLs for different data types.

### Key Features Showcased
- **In-Memory Caching**: Simple cache with LRU eviction and TTL (caching_demo.py)
- **Redis Cloud Integration**: Distributed caching with connection pooling and TLS support
- **Multi-Layer Architecture**: Primary Redis + automatic file cache fallback
- **Circuit Breaker**: Graceful degradation when Redis is unavailable
- **Optimized TTLs**: Different expiration times per data type (24h catalogs, 7d embeddings, 1h searches)
- **Performance Metrics**: Real-world benchmarks showing 1.5-1.6x speedup with 80%+ hit rates

### Real-World Impact

| Scenario | Without Cache | With Cache (80% hit rate) | Improvement |
|----------|---------------|---------------------------|-------------|
| **Latency** | 800-2500ms | 0-20ms (hits), 140ms (misses) | **87% faster** |
| **Cost** | $0.26/workflow | $0.09/workflow | **65% savings** |
| **API Calls** | Every request | Only on cache miss | **80% reduction** |

> **Note:** Performance varies based on cache hit rate (30-90%) depending on query patterns and workload repetition.

## Setup

### Prerequisites
```bash
# Required: python-dotenv for configuration
pip install python-dotenv

# Optional: redis package (only if USE_REDIS=true in .env)
pip install redis
```

### Configuration

Copy `.env.example` to `.env` and configure:
```bash
cp .env.example .env
```

**Minimum Configuration** (Azure OpenAI only):
```env
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key-here
AZURE_OPENAI_API_VERSION=2024-08-01-preview
PLANNER_PROVIDER=azure-openai
PLANNER_MODEL=gpt-4o

# Mock cache mode (no Redis needed)
USE_REDIS=false
```

**Optional: Real Redis** (for production patterns):
```env
# Enable real Redis connection
USE_REDIS=true
REDIS_URL=redis://redis-xxxxx.your-cloud-provider.com:10485
REDIS_PASSWORD=your-redis-password-here
```

### Running the Samples

All samples work in two modes controlled by the `USE_REDIS` environment variable:

| Mode | `USE_REDIS` | Behavior | Requirements |
|------|-------------|----------|--------------|
| **Mock** | `false` | File-based cache with Redis fallback pattern demo | Python only |
| **Real** | `true` | Connects to real Redis with automatic file fallback | Redis instance + redis package |

**Windows Users:** If you see encoding errors, set UTF-8:
```powershell
$env:PYTHONIOENCODING="utf-8"
python caching_demo.py
```

## Files

| File | Purpose | Cache Type | Runtime |
|------|---------|------------|---------|
| `caching_demo.py` | Basic caching patterns and metrics | In-memory (SimpleCache) | ~3 seconds |
| `caching_deep_dive.py` | Multi-layer architecture deep dive | Redis OR file fallback (toggle via USE_REDIS) | ~10 seconds |
| `caching_what_how_when.py` | Complete technical guide (WHAT/HOW/WHEN) | Redis OR file fallback (toggle via USE_REDIS) | ~5 seconds |
| `.env` | Your actual configuration | - | - |
| `.env.example` | Configuration template | - | - |

**All samples respond to `USE_REDIS` setting:**
- When `true`: Connect to real Redis, show production latencies
- When `false`: Use file cache, show mock latencies

## Example Walkthroughs

### üéØ Example 1: caching_demo.py - Basic In-Memory Caching

**Concept**: Simple in-memory caching with LRU eviction and TTL

**What You'll Learn**:
- How to implement a basic cache using Python's `OrderedDict`
- LRU (Least Recently Used) eviction strategy
- TTL (Time To Live) expiration
- Tracking cache hits/misses and calculating hit rates
- Measuring performance improvements

**Key Implementation**:
```python
# Cache lookup pattern
cached = cache.get(cache_key)
if cached:
    return cached  # HIT - instant response (0ms)
    
# Cache miss - do expensive operation
result = await expensive_operation()  # 800ms
cache.set(cache_key, result)  # Save for next time
```

**Output Highlights**:
- ‚úÖ Hit rate: 81.8% (18 hits out of 22 requests)
- ‚úÖ Speedup: 1.59x faster with caching
- ‚úÖ Cost savings: $0.43 (63.6% reduction)
- ‚úÖ Latency: 0ms (hit) vs 800ms (miss)

**Best For**: Understanding caching fundamentals before moving to distributed caching

---

### üéØ Example 2: caching_deep_dive.py - Multi-Layer Architecture

**Concept**: Production-grade caching that works WITH or WITHOUT Redis

**What You'll Learn**:
- Multi-layer cache architecture (Redis ‚Üí Memory ‚Üí File fallback)
- Circuit breaker pattern for graceful degradation
- Health checks and monitoring
- How the same code works in both modes (toggle USE_REDIS)

**Two Modes - Same Code, Different Backends**:

| Scenario | `USE_REDIS` | What You'll See | Latency |
|----------|-------------|-----------------|---------|
| **With Redis** | `true` | Status: ‚úì Connected (using Redis)<br>SET: 18-20ms via Redis<br>GET: 16-23ms via Redis | Production latency |
| **Without Redis** | `false` | Status: ‚ùå Not configured (using file cache)<br>SET: 0.25ms via memory<br>GET: 0.00ms via memory | Mock mode (instant) |

**Try It Yourself**:
```bash
# Run with Redis
echo "USE_REDIS=true" >> .env
python caching_deep_dive.py

# Run without Redis
echo "USE_REDIS=false" >> .env
python caching_deep_dive.py
```

Both show the exact same caching patterns - just different performance characteristics!
- ToolCache high-level API with optimized TTLs
- How to toggle between mock and real Redis

**7 Demonstrations**:

1. **Initialization** - Shows health status and cache configuration
   ```
   Redis Available: False (USE_REDIS=false)
   Redis URL: ‚ùå Not configured (using mock cache)
   Circuit Breaker: CLOSED
   Fallback Enabled: True
   Cache Directory: ~/.toolweaver/cache_demo
   ```

2. **Basic Operations** - SET/GET with timing
   ```
   SET user:123 ‚Üí 0.27ms
   GET user:123 ‚Üí {data} (0.00ms)
   GET nonexistent ‚Üí None (MISS)
   ```

3. **TTL Expiration** - Sets 2s TTL, verifies expiration after 3s
   ```
   GET temp:data ‚Üí "value" (immediate)
   GET temp:data ‚Üí None (after 3s - expired)
   ```

4. **Complex Data** - Caches tool catalogs, search results, embeddings
   ```python
   cache.set("catalog:v2:hash", {
       "tools": {...},
       "version": "v2"
   }, ttl=86400)  # 24 hours
   ```

5. **ToolCache API** - High-level wrapper with domain-specific TTLs
   ```python
   tool_cache.set_catalog(hash, catalog)      # 24h TTL
   tool_cache.set_embedding(hash, model, emb) # 7d TTL
   tool_cache.set_search_results(hash, results) # 1h TTL
   ```

6. **Circuit Breaker** - Simulates failures, shows automatic fallback
   ```
   Failure 1: State = CLOSED
   Failure 2: State = CLOSED
   Failure 3: State = OPEN  ‚Üê Switches to file cache
   ```

7. **Performance Comparison** - Measures speedup with cache hits
   ```
   Request 1: 141ms (MISS)
   Request 2: 136ms (MISS)
   Request 3: 18ms (HIT) ‚Üê 7.5x faster!
   Request 4: 14ms (HIT)
   Request 5: 16ms (HIT)
   Speedup: 1.6x overall
   ```

**Architecture**:
```
Your App ‚Üí ToolCache (high-level API)
              ‚Üì
           RedisCache (mock or real)
              ‚Üì
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚Üì                 ‚Üì
  Memory Cache    File Cache
  (fast, temp)    (fallback, persistent)
```

**When to use each mode**:
- **Mock mode** (`USE_REDIS=false`): Learning, demos, CI/CD, local dev without Redis
- **Real mode** (`USE_REDIS=true`): Production, distributed systems, sharing cache across services

**Best For**: Understanding production caching patterns and resilience strategies
      Redis Cloud (primary) ‚Üê If fails...
              ‚Üì
      File Cache (fallback) ‚Üê ...uses this
              ‚Üì
      Circuit Breaker (prevents retry storms)
```

**Best For**: Understanding production caching architecture and resilience patterns

---

### üéØ Example 3: caching_what_how_when.py - Technical Deep Dive

**Concept**: Complete technical guide showing implementation details

**What You'll Learn**:
- Exactly what data structures are cached
- How cache keys are generated (SHA-256 hashing)
- When caching happens in the request flow
- Cache invalidation strategies
- Performance measurements

**5 Demonstrations**:

#### 1. **WHAT is cached** - Shows all 5 data types with examples
```python
# Tool Catalog
Key: "catalog:v2:953c5d322c80bb69"
Data: {"tools": {...}, "version": "v2"}
TTL: 24 hours
Why: Tools rarely change; expensive to discover

# Embeddings
Key: "embedding:afff376ee22c5cc8:ada-002"
Data: [0.023, -0.145, ...] # 1536-dim vector
TTL: 7 days
Why: Most expensive operation ($0.0001/call)

# Search Results, Tool Metadata, Idempotency...
```

#### 2. **HOW it works** - Implementation details
```python
# Key generation using SHA-256 hash
hash = sha256(json.dumps(data, sort_keys=True)).hexdigest()[:16]
key = f"catalog:v2:{hash}"

# Write path: Redis ‚Üí File ‚Üí Circuit breaker
cache.set(key, data, ttl=86400)
  ‚Üí Try Redis first (serialize with pickle)
  ‚Üí On failure: Write to file cache
  ‚Üí Circuit breaker: Track failures

# Read path: Same dual-layer approach
result = cache.get(key)
  ‚Üí Try Redis first
  ‚Üí On miss/failure: Try file cache
  ‚Üí Return None if both miss
```

#### 3. **WHEN it happens** - Request flow timeline

**Scenario 1: Tool Discovery**
```
User calls: discover_tools(use_cache=True)
  ‚Üí Check cache: ~/.toolweaver/tool_cache.json
  
  IF HIT:
    ‚úì Return cached catalog immediately
    ‚úì No network calls
    ‚úì ~5ms response time
  
  IF MISS:
    ‚§∑ Run all discoverers (MCP, filesystem, A2A)
    ‚§∑ Aggregate results (~2000ms)
    ‚§∑ CACHE for 24 hours
```

**Scenario 2: Semantic Search**
```
User calls: search_tools("process receipt")
  ‚Üí Generate embedding
    ‚Üí Check cache: embedding:{hash}:ada-002 (7d TTL)
    ‚Üí IF MISS: Call OpenAI API ‚Üí Cache result
  
  ‚Üí Search vector database
    ‚Üí Check cache: search:{hash}:v2:10 (1h TTL)
    ‚Üí IF MISS: Execute search ‚Üí Cache results
```

**Scenario 3: Agent Dispatch (Idempotency)**
```
User calls: dispatch_agents(tasks=[task1, task2])
  ‚Üí For each task:
    ‚Üí Generate key: hash(agent + template + args)
    ‚Üí Check cache: {idempotency_key} (1h TTL)
    
    IF HIT: Return cached result (prevents duplicate execution)
    IF MISS: Execute task ‚Üí Cache result
```

#### 4. **Cache Invalidation** - How data expires
- **TTL-based** (automatic): Data expires after specified time
- **Version-based**: Tool changes ‚Üí version bump ‚Üí new cache key
- **Manual**: `cache.delete(key)` or `cache.clear()`

#### 5. **Performance Comparison** - Real benchmarks
```
Without cache (10 operations): 1.08s
With cache (50% hit rate):    0.54s
Speedup: 2x faster

Cost without cache: $0.50
Cost with cache:    $0.25 (50% savings)
```

**Best For**: Deep technical understanding of cache implementation and optimization

---

## Which Example Should I Run?

| Your Goal | Run This | What You'll Learn |
|-----------|----------|-------------------|
| **Learning caching basics** | `caching_demo.py` | Cache hits/misses, LRU, TTL, metrics |
| **Understanding Redis architecture** | `caching_deep_dive.py` | Multi-layer, circuit breaker, fallback |
| **Implementation details** | `caching_what_how_when.py` | Key generation, request flow, invalidation |
| **All of the above** | Run in order 1‚Üí2‚Üí3 | Complete caching knowledge |

**Recommended Path**: Start with #1 for concepts ‚Üí #2 for architecture ‚Üí #3 for deep dive

## Run the Samples

**Start with the basic demo:**
```bash
# In-memory caching (no Redis required)
python caching_demo.py
```

**Then explore Redis Cloud integration:**
```bash
# Redis architecture and performance benchmarks
python caching_deep_dive.py

# Complete walkthrough: what, how, and when caching happens
python caching_what_how_when.py
```

### Which Sample Should I Run?

- **Learning caching basics?** ‚Üí Start with `caching_demo.py`
- **Understanding Redis architecture?** ‚Üí Run `caching_deep_dive.py`
- **Need implementation details?** ‚Üí Check `caching_what_how_when.py`

## Test Results

### ‚úÖ caching_demo.py (In-Memory Cache)
Demonstrates basic caching patterns with SimpleCache (LRU + TTL):
- **Hit Rate**: 81.8% (18 hits / 22 total requests)
- **Speedup**: 1.59x faster overall
- **Cost Savings**: $0.43 saved (63.6% reduction)
- **Latency**: 0ms (cache hit) vs 800ms (cache miss)

**Key Learning**: Even simple in-memory caching provides significant benefits

### ‚úÖ caching_deep_dive.py (Redis Cloud)
Production-grade multi-layer architecture with Redis Cloud:
- **Connection**: ‚úì Successfully connected to Redis Cloud
- **Speedup**: 1.6x with cache hits (18ms hit vs 141ms miss)
- **Cache Operations**: 15-18ms for SET/GET to Redis Cloud
- **Circuit Breaker**: ‚úì Automatic fallback to file cache on failures
- **TTL Expiration**: ‚úì Data expires correctly after specified time
- **Fallback**: ‚úì File cache activates when Redis unavailable

**Key Learning**: Redis provides distributed caching with automatic resilience

### ‚úÖ caching_what_how_when.py (Redis Cloud)
Technical deep dive showing implementation details:
- **Connection**: ‚úì Successfully connected to Redis Cloud  
- **What's Cached**: Catalogs, search results, embeddings, tool metadata, agent results
- **How It Works**: Dual-layer (Redis + file), circuit breaker, hash-based keys
- **When It Happens**: Discovery, semantic search, agent dispatch, tool loading
- **Cache Layers**:
  - Tool Catalog: 24h TTL (rarely changes)
  - Search Results: 1h TTL (dynamic but stable)
  - Embeddings: 7d TTL (expensive to compute)
  - Tool Metadata: 24h TTL (versioned definitions)

**Key Learning**: Understanding cache architecture helps optimize for your workload

## What Is Being Cached (And Why)

### 1. Tool Catalog (24h TTL) - ‚≠ê‚≠ê‚≠ê‚≠ê High Priority
**What**: Complete snapshot of all discovered tools
```python
{
  "tools": {"process_receipt": {...}, "categorize": {...}},
  "version": "v2",
  "discovered_at": "2026-01-07T10:00:00"
}
```
**Why Cached**: 
- Tool discovery scans multiple sources (MCP servers, filesystem, A2A endpoints)
- Discovery is expensive (~2s for full scan across all discoverers)
- Tools rarely change once deployed
- Prevents redundant discovery on every request
- **Invalidation**: Version-based (new tools ‚Üí version bump ‚Üí new cache key)

### 2. Embeddings (7d TTL) - ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Critical Priority
**What**: Vector representations of text (300-1536 dimensions)
```python
# text-embedding-ada-002 produces 1536-dim vectors
[0.023, -0.145, 0.089, ..., 0.067]  # 1536 float values
```
**Why Cached**: 
- **Most expensive operation** (OpenAI API: ~$0.0001 per request, adds up fast)
- Embeddings are **deterministic** (same text ‚Üí always same vector)
- Used for semantic search, tool matching, query understanding
- Long TTL (7 days) justified because embeddings never change for given text
- **Invalidation**: Hash-based key (text changes ‚Üí hash changes ‚Üí new cache)

### 3. Search Results (1h TTL) - ‚≠ê‚≠ê‚≠ê Medium Priority
**What**: Semantic search results for a query
```python
[
  {"tool": "process_receipt", "score": 0.95, "reason": "Matches OCR"},
  {"tool": "categorize", "score": 0.87, "reason": "Item classification"}
]
```
**Why Cached**:
- Same queries return same results within catalog version
- Avoids re-computing vector similarity search on every request
- Query patterns tend to repeat in workflows (e.g., "process receipt" appears frequently)
- 1h TTL balances freshness vs performance
- **Invalidation**: Includes catalog version in key (tools change ‚Üí version bump ‚Üí new search)

### 4. Tool Metadata (24h TTL) - ‚≠ê‚≠ê Low Priority
**What**: Individual tool definitions
```python
{
  "name": "process_receipt",
  "version": "1.0",
  "description": "Extract and process receipt data",
  "parameters": {"image_uri": {"type": "string"}}
}
```
**Why Cached**:
- Quick access to specific tools without loading full catalog
- Versioned definitions rarely change
- Reduces registry lookups during tool execution
- **Invalidation**: Version-based (tool update ‚Üí version bump ‚Üí new cache key)

### 5. Agent Results / Idempotency (1h TTL) - ‚≠ê‚≠ê‚≠ê Medium Priority
**What**: Results from completed agent operations
```python
{
  "task_id": "3c1585f4a2258f6d",
  "output": "Receipt processed: Total $45.67",
  "status": "completed",
  "timestamp": "2026-01-07T10:30:00"
}
```
**Why Cached**:
- **Prevent duplicate operations** on retries (critical for reliability)
- Network failures shouldn't cause expensive re-execution
- Idempotency guarantees for distributed systems
- 1h TTL covers typical retry windows
- **Invalidation**: Time-based expiration after retry window

### Cache Priority Matrix

| Data Type | Cost to Generate | Change Frequency | TTL | Cache Priority | Impact |
|-----------|------------------|------------------|-----|----------------|--------|
| **Embeddings** | $$$ High ($0.0001/call) | Never | 7d | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Critical | 90% cost savings |
| **Tool Catalog** | $$ Medium (2s scan) | Rarely | 24h | ‚≠ê‚≠ê‚≠ê‚≠ê High | 87% faster discovery |
| **Search Results** | $ Low (vector search) | Per catalog | 1h | ‚≠ê‚≠ê‚≠ê Medium | 65% faster searches |
| **Tool Metadata** | $ Low (registry lookup) | Rarely | 24h | ‚≠ê‚≠ê Low | Marginal gains |
| **Idempotency** | N/A (operational) | Once | 1h | ‚≠ê‚≠ê‚≠ê Medium | Prevents duplicates |

### Why This Caching Strategy?

1. **Cost Optimization**: Embeddings are most expensive ‚Üí longest TTL (7 days)
2. **Performance**: Discovery is slowest operation ‚Üí cache for 24 hours
3. **Consistency**: Search results cached per catalog version ‚Üí no stale matches
4. **Reliability**: Idempotency prevents duplicate operations on retries
5. **Scalability**: Distributed Redis cache shared across all servers
6. **Trade-off**: Accepts low stale data risk (tools change infrequently) for massive performance/cost gains

**Real-World Impact**: In production with 1000 requests/day:
- Without cache: 1000 embedding calls √ó $0.0001 = **$0.10/day** = **$36.50/year**
- With cache (90% hit rate): 100 calls √ó $0.0001 = **$0.01/day** = **$3.65/year**
- **Savings: $32.85/year** (just for embeddings; multiply by all cache layers)

## Architecture Highlights

### Multi-Layer Caching Strategy
```
Request ‚Üí Redis (Primary)
           ‚Üì (on failure)
       File Cache (Fallback)
           ‚Üì
     Circuit Breaker (Protection)
```

**Components:**
1. **RedisCache** - Primary distributed cache with connection pooling (10 connections)
2. **File Cache** - Automatic fallback using pickle serialization
3. **Circuit Breaker** - Prevents retry storms (opens after 3-5 failures, 60s recovery)
4. **ToolCache** - High-level API with domain-specific TTLs

### Cache Layers & TTL Strategy

| Data Type | TTL | Rationale |
|-----------|-----|-----------|
| Tool Catalog | 24 hours | Tools rarely change; expensive to discover |
| Search Results | 1 hour | Query patterns repeat; embeddings are stable |
| Embeddings | 7 days | Most expensive operation; deterministic results |
| Tool Metadata | 24 hours | Versioned definitions; infrequent updates |
| Idempotency | 1 hour | Prevent duplicate operations on retries |

### Deployment Options

| Option | Cost | Use Case | Configuration |
|--------|------|----------|---------------|
| **File Cache Only** | $0 | Development, single server | `enable_fallback=True` (no Redis URL) |
| **Local Docker Redis** | $0 | Development, testing | `redis://localhost:6379` |
| **Redis Cloud Free** | $0 | Small production, 30MB | Current `.env` config |
| **Redis Cloud Paid** | $7+/mo | Production, 100MB+ | Update `.env` with paid tier URL |
| **Azure Cache for Redis** | $18+/mo | Enterprise, Azure integration | `rediss://...cache.windows.net:6380` |

### Benefits Summary

‚úÖ **Performance**: 1.5-1.6x speedup (actual test results)  
‚úÖ **Cost Reduction**: 65-90% savings through cache hits  
‚úÖ **Resilience**: Automatic fallback if Redis unavailable  
‚úÖ **Scalability**: Distributed cache shared across servers  
‚úÖ **Zero Downtime**: Circuit breaker prevents cascading failures